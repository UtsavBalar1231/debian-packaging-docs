name: Update Debian Wiki Documentation

on:
  # Run daily at 2:00 AM UTC
  schedule:
    - cron: '0 2 * * *'

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      max_depth:
        description: 'Maximum scraping depth'
        required: false
        default: '10'
      delay:
        description: 'Delay between requests (seconds)'
        required: false
        default: '1.5'

permissions:
  contents: write

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0
        with:
          fetch-depth: 0  # Full history for better git operations

      - name: Set up Python
        uses: actions/setup-python@e797f83bcb11b83ae66e0230d6156d7c80228e7c # v6.0.0
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@85856786d1ce8acfbcc2f13a5f3fbd6b938f9f41 # v7.1.2
        with:
          enable-cache: true

      - name: Install dependencies
        run: uv sync

      - name: Run Debian Wiki scraper
        id: scrape
        run: |
          echo "Starting scrape at $(date)"

          # Use workflow inputs if provided, otherwise use defaults
          MAX_DEPTH="${{ github.event.inputs.max_depth || '10' }}"
          DELAY="${{ github.event.inputs.delay || '1.5' }}"

          uv run python scraper.py \
            --max-depth "$MAX_DEPTH" \
            --delay "$DELAY" \
            --output docs

          echo "Scrape completed at $(date)"
        continue-on-error: false

      - name: Check for changes
        id: git-check
        run: |
          git add docs/

          # Check if there are any changes to commit
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No changes detected in documentation"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT

            # Count changed files
            CHANGED_FILES=$(git diff --staged --numstat | wc -l)
            echo "changed_files=$CHANGED_FILES" >> $GITHUB_OUTPUT
            echo "Detected changes in $CHANGED_FILES file(s)"

            # Show summary of changes
            echo "### Changes Summary" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            git diff --staged --stat >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Commit and push changes
        if: steps.git-check.outputs.has_changes == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Create commit with date
          COMMIT_DATE=$(date +"%Y-%m-%d")
          git commit -m "docs: update Debian wiki documentation ($COMMIT_DATE)" \
                     -m "Automated update via GitHub Actions" \
                     -m "Changed files: ${{ steps.git-check.outputs.changed_files }}"

          git push

          echo "[SUCCESS] Changes committed and pushed successfully" >> $GITHUB_STEP_SUMMARY

      - name: No changes
        if: steps.git-check.outputs.has_changes == 'false'
        run: |
          echo "[INFO] No documentation updates found" >> $GITHUB_STEP_SUMMARY

      - name: Upload scraper log
        if: always()
        uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4 # v5.0.0
        with:
          name: scraper-log-${{ github.run_number }}
          path: scraper.log
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Date**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- **Max Depth**: ${{ github.event.inputs.max_depth || '10' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Delay**: ${{ github.event.inputs.delay || '1.5' }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Changes Detected**: ${{ steps.git-check.outputs.has_changes }}" >> $GITHUB_STEP_SUMMARY

          if [ -f scraper.log ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Scraper Statistics" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            tail -n 20 scraper.log | grep -E "(Pages scraped|Pages failed|Pages skipped|Total URLs)" || echo "Statistics not available"
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
